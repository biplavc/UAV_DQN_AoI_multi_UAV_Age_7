{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import UserList\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import datetime\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# from tf_environment import *\n",
    "# from comet_ml import Experiment\n",
    "\n",
    "# experiment = Experiment(\"HsbMT2nT816RPUXC1LLkVvEe0\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "from create_graph_1 import *\n",
    "# from path_loss_probability import *\n",
    "import itertools\n",
    "from itertools import product  \n",
    "from tf_reinforce import *\n",
    "from tf_dqn import *\n",
    "from tf_c51 import *\n",
    "from tf_sac import *\n",
    "from random_scheduling import *\n",
    "from greedy_scheduling import *\n",
    "from mad_scheduling import *\n",
    "\n",
    "import sys\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "\n",
    "from parameters import *\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def distributed_run(arguments):\n",
    "  \n",
    "    print(f\"passed arguments is {arguments}\")\n",
    "    pool.starmap(do_scheduling, [(arg[0], arg[1], arg[2]) for arg in arguments])\n",
    "    \n",
    "#############################################################\n",
    "\n",
    "def do_scheduling(deployment, scheduler, I):\n",
    "    \n",
    "    I = I.cuda()\n",
    "    deployment_options = [\"MDS\", \"RP\"]\n",
    "    scheduler_options  = [\"random\", \"greedy\", \"MAD\", \"dqn\", \"c51\"]\n",
    "    assert(deployment in deployment_options and scheduler in scheduler_options)\n",
    "\n",
    "    random.seed(42) ## this seed ensures same location of users in every case, keep both seeds\n",
    "    \n",
    "    if test_case:\n",
    "\n",
    "        drones_needed       = 1\n",
    "        drones_coverage     = [[1,2,3,4,5,6]] ##[[1,2,3,4,5,6,7,8,9,10]]\n",
    "        periodicity         = {1:2, 2:3, 3:4, 4:2, 5:3, 6:4} ##, 7:4, 8:3, 9:4, 10:3}\n",
    "\n",
    "        user_list = []\n",
    "        UAV_list = np.arange(drones_needed)\n",
    "        for i in drones_coverage:\n",
    "            for j in i:\n",
    "                if j!=0:\n",
    "                    user_list.append(j)\n",
    "        \n",
    "        I = len(user_list)\n",
    "\n",
    "        if packet_loss == True:\n",
    "            packet_update_loss  = {yy : round(random.random(),2) for yy in user_list}\n",
    "            packet_sample_loss  = {yy : round(random.random(),2) for yy in user_list}\n",
    "        else:\n",
    "            packet_update_loss  = {yy : -1 for yy in user_list}\n",
    "            packet_sample_loss  = {yy : -1 for yy in user_list}\n",
    "            \n",
    "    else: ## user defined UAV and user configuration\n",
    "                        \n",
    "        # I is number of users, L length and B breadth\n",
    "        x_vals = random.sample(range(1, L-1), I) # x-coordinates for users\n",
    "        y_vals = random.sample(range(1, B-1), I) # y-coordinates for users\n",
    "        z_vals = [0]*I\n",
    "\n",
    "        user_coordinates = list(zip(x_vals,y_vals))\n",
    "\n",
    "        x_grid_nos = int(L/r) + 1 # number of different values the grid takes for x axis\n",
    "        y_grid_nos = int(B/r) + 1 # number of different values the grid takes for y axis\n",
    "\n",
    "        grid_x = np.linspace(0, L, num = x_grid_nos) # generate evenly spaced x positions for grid\n",
    "        grid_y = np.linspace(0, B, num = y_grid_nos) # generate evenly spaced y positions for grid\n",
    "        \n",
    "        grid_coordinates = list(itertools.product(grid_x , grid_y))\n",
    "\n",
    "        # print(f\"user_coordinates = {user_coordinates}, grid_coordinates = {grid_coordinates}, deployment = {deployment}\") \n",
    "        # print(f'coverage calculated {deployment} deployment for {I} users under {scheduler} scheduling - user_coordinates = {user_coordinates}', file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
    "        drones_needed, drones_coverage = create_graph_1(user_coordinates, grid_coordinates, deployment)\n",
    "        # drones[deployment].append([I, drones_needed])\n",
    "        \n",
    "    \n",
    "        user_list = [] ## this is not the same user_list as defined in the environment, this is just used to index the packet loss and sample loss\n",
    "        UAV_list  = np.arange(drones_needed)\n",
    "        \n",
    "        for i in drones_coverage:\n",
    "            for j in i:\n",
    "                if j!=0:\n",
    "                    user_list.append(j)\n",
    "                    \n",
    "        periodicity = {x:np.random.choice([2,3,4]) for x in user_list}\n",
    "\n",
    "\n",
    "        if packet_loss == True:\n",
    "            packet_update_loss = {yy : round(random.random(),2) for yy in user_list}\n",
    "            packet_sample_loss = {yy : round(random.random(),2) for yy in user_list}\n",
    "        else:\n",
    "            packet_update_loss = {yy : -1 for yy in user_list}\n",
    "            packet_sample_loss = {yy : -1 for yy in user_list}\n",
    "            \n",
    "            \n",
    "    # print(f\"periodicity = {periodicity}\")\n",
    "    # time.sleep(10)\n",
    "\n",
    "    print(f\"\\n\\n{deployment} deployment for {I} users under {scheduler} scheduling\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
    "\n",
    "            \n",
    "    print(f'Under test_case = {test_case}, drones_needed = {drones_needed}, UAV_list = {UAV_list}, drones_coverage = {drones_coverage}, user_list = {user_list}, periodicity = {periodicity} for {deployment} deployment for {I} users under {scheduler} scheduling, update loss = {packet_update_loss}, sampling loss = {packet_sample_loss}, user_list = {user_list}, UAV_list = {UAV_list}, CSI_as_state = {CSI_as_state}, sample_error_in_CSI = {sample_error_in_CSI}', file=open(folder_name + \"/results.txt\", \"a\"), flush=True)  \n",
    "    \n",
    "\n",
    "    str_x = str(deployment) + \" placement with \" + str(I) + \" users needs \" + str(scheduler) + \" scheduler and \"  + str(drones_needed) + \" drones\\n\"\n",
    "    print(f'{str_x}', file=open(folder_name + \"/drones.txt\", \"a\"), flush=True)\n",
    "    \n",
    "    \n",
    "    if scheduler == \"greedy\":\n",
    "        greedy_overall[I], greedy_final[I], greedy_all_actions[I] = greedy_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity)  \n",
    "        pickle.dump(greedy_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_overall.pickle\", \"wb\")) \n",
    "        pickle.dump(greedy_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_greedy_final.pickle\", \"wb\"))\n",
    "        pickle.dump(greedy_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"_greedy_all_actions.pickle\", \"wb\")) \n",
    "    \n",
    "    if scheduler == \"random\":\n",
    "        random_overall[I], random_final[I], random_all_actions[I] = random_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity)\n",
    "        pickle.dump(random_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_overall.pickle\", \"wb\")) \n",
    "        pickle.dump(random_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_final.pickle\", \"wb\")) \n",
    "        pickle.dump(random_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_random_all_actions.pickle\", \"wb\")) \n",
    "        \n",
    "        \n",
    "    if scheduler == \"MAD\":\n",
    "        mad_overall[I], mad_final[I], mad_all_actions[I] = mad_scheduling(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity)\n",
    "        pickle.dump(mad_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_overall.pickle\", \"wb\")) \n",
    "        pickle.dump(mad_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_final.pickle\", \"wb\"))\n",
    "        pickle.dump(mad_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_mad_all_actions.pickle\", \"wb\")) \n",
    "        \n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    if scheduler == \"dqn\":\n",
    "        dqn_overall[I], dqn_final[I], dqn_all_actions[I] = tf_dqn(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity)\n",
    "        t3 = time.time()\n",
    "        print(\"DQN for \", I, \" users took \", t3-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
    "        pickle.dump(dqn_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_overall.pickle\", \"wb\")) \n",
    "        pickle.dump(dqn_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_final.pickle\", \"wb\"))\n",
    "        pickle.dump(dqn_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_dqn_all_actions.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "    if scheduler == \"c51\":\n",
    "        c51_overall[I], c51_final[I], c51_all_actions[I] = tf_c51(I, drones_coverage, folder_name, deployment, packet_update_loss, packet_sample_loss, periodicity)\n",
    "        t4 = time.time()\n",
    "        print(\"c51 for \", I, \" users took \", t4-t1, \" seconds to complete\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
    "        pickle.dump(c51_overall, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_overall.pickle\", \"wb\")) \n",
    "        pickle.dump(c51_final, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_final.pickle\", \"wb\")) \n",
    "        pickle.dump(c51_all_actions, open(folder_name + \"/\" + deployment + \"/\" + str(I) + \"U_c51_all_actions.pickle\", \"wb\")) \n",
    "        \n",
    "\n",
    "    print(f\"{I} users under {scheduler} scheduling and {deployment} placement are over\\n\\n\", file=open(folder_name + \"/results.txt\", \"a\"), flush=True)\n",
    "    print(f\"{I} users under {scheduler} scheduling and {deployment} placement are over\\n\\n\")\n",
    "\n",
    "#############################################################\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    now_str_1 = now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    folder_name = 'models/' +  now_str_1\n",
    "    \n",
    "    folder_name_MDS = folder_name + \"/MDS\"\n",
    "    folder_name_random = folder_name + \"/RP\" ## RP means random placement\n",
    "\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        os.makedirs(folder_name_MDS)\n",
    "        os.makedirs(folder_name_random)\n",
    "        \n",
    "        \n",
    "    print(\"execution started at \", now_str_1, file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
    "\n",
    "    print(\"num_iterations = \",num_iterations, \", random_episodes = \", random_episodes,\", BS_capacity = \", BS_capacity, \", UAV_capacity = \", UAV_capacity,\",  MAX_STEPS = \", MAX_STEPS, \" gamma = \", set_gamma, \", learning_rate = \", learning_rate, \", fc_layer_params = \", fc_layer_params, \", replay_buffer_capacity = \", replay_buffer_capacity, \", coverage_capacity = \", coverage_capacity, \", L = \", L, \", B = \", B, \", R = \", R, \", r = \", r,  \"\\n\", file = open(folder_name + \"/results.txt\", \"a\"), flush = True)\n",
    "\n",
    "    users       = [6] ## if test case, change this\n",
    "\n",
    "    deployments = [\"RP\"] #, \"RP\"] #, \"MDS\"]\n",
    "    \n",
    "    schedulers  = [\"dqn\", \"MAD\", \"random\", \"greedy\"]\n",
    "\n",
    "        \n",
    "    test_case = False\n",
    "    # test_case = True\n",
    "    \n",
    "    # packet_loss = False\n",
    "    packet_loss = True\n",
    "\n",
    "\n",
    "    arguments = list(itertools.product(deployments, schedulers, users))\n",
    "    \n",
    "    \n",
    "    dqn_overall = {}\n",
    "    dqn_final = {}\n",
    "    dqn_all_actions = {}\n",
    "    \n",
    "    c51_overall = {}\n",
    "    c51_final = {}\n",
    "    c51_all_actions = {}\n",
    "    \n",
    "    reinforce_overall = {}\n",
    "    reinforce_final = {}\n",
    "    reinforce_all_actions = {}\n",
    "    \n",
    "    random_overall = {} ## sum of age at BS for all of the MAX_STEPS time steps\n",
    "    random_final   = {} ## sum of age at BS for step =  MAX_STEPS i.e. last time step\n",
    "    random_all_actions = {}\n",
    "    \n",
    "    greedy_overall = {}\n",
    "    greedy_final   = {}\n",
    "    greedy_all_actions = {}\n",
    "    \n",
    "    mad_overall = {}\n",
    "    mad_final   = {}\n",
    "    mad_all_actions = {}\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    print(f\"pool is {pool}\", file = open(folder_name + \"/results.txt\", \"a\"))\n",
    "    distributed_run(arguments)\n",
    "\n",
    "    pool.close()    \n"
   ]
  }
 ]
}